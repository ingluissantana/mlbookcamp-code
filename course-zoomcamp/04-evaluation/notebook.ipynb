{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb05700",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b9b4a",
   "metadata": {},
   "source": [
    "# 4. Evaluation Metrics for Classification\n",
    "\n",
    "In the previous session we trained a model for predicting churn. How do we know if it's good?\n",
    "\n",
    "\n",
    "## 4.1 Evaluation metrics: session overview \n",
    "\n",
    "* Dataset: https://www.kaggle.com/blastchar/telco-customer-churn\n",
    "* https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
    "\n",
    "\n",
    "*Metric* - function that compares the predictions with the actual values and outputs a single number that tells how good the predictions are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff7c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data-week-3.csv')\n",
    "\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "\n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    "\n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    "\n",
    "df.churn = (df.churn == 'yes').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1903b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "y_train = df_train.churn.values\n",
    "y_val = df_val.churn.values\n",
    "y_test = df_test.churn.values\n",
    "\n",
    "del df_train['churn']\n",
    "del df_val['churn']\n",
    "del df_test['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4132a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    "\n",
    "categorical = [\n",
    "    'gender',\n",
    "    'seniorcitizen',\n",
    "    'partner',\n",
    "    'dependents',\n",
    "    'phoneservice',\n",
    "    'multiplelines',\n",
    "    'internetservice',\n",
    "    'onlinesecurity',\n",
    "    'onlinebackup',\n",
    "    'deviceprotection',\n",
    "    'techsupport',\n",
    "    'streamingtv',\n",
    "    'streamingmovies',\n",
    "    'contract',\n",
    "    'paperlessbilling',\n",
    "    'paymentmethod',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4583b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "train_dict = df_train[categorical + numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea51f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)\n",
    "\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "churn_decision = (y_pred >= 0.5)\n",
    "(y_val == churn_decision).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8a7559",
   "metadata": {},
   "source": [
    "## 4.2 Accuracy and dummy model\n",
    "\n",
    "* Evaluate the model on different thresholds\n",
    "* Check the accuracy of dummy baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948eda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc59dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_val == churn_decision).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcb4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "1132/ 1409"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb75fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_val, y_pred >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27840301",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 21)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    score = accuracy_score(y_val, y_pred >= t)\n",
    "    print('%.2f %.3f' % (t, score))\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f492a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresholds, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0897055",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_pred >= 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ece7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - y_val.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1600912",
   "metadata": {},
   "source": [
    "## 4.3 Confusion table\n",
    "\n",
    "* Different types of errors and correct decisions\n",
    "* Arranging them in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501711e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_positive = (y_val == 1)\n",
    "actual_negative = (y_val == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d898b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.5\n",
    "predict_positive = (y_pred >= t)\n",
    "predict_negative = (y_pred < t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = (predict_positive & actual_positive).sum()\n",
    "tn = (predict_negative & actual_negative).sum()\n",
    "\n",
    "fp = (predict_positive & actual_negative).sum()\n",
    "fn = (predict_negative & actual_positive).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.array([\n",
    "    [tn, fp],\n",
    "    [fn, tp]\n",
    "])\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7aab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "(confusion_matrix / confusion_matrix.sum()).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b1d4c",
   "metadata": {},
   "source": [
    "## 4.4 Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfcf399",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tp / (tp + fp)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e180eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tp / (tp + fn)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088b30e",
   "metadata": {},
   "source": [
    "## 4.5 ROC Curves\n",
    "\n",
    "### TPR and FRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = tp / (tp + fn)\n",
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = fp / (fp + tn)\n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "\n",
    "for t in thresholds:\n",
    "    actual_positive = (y_val == 1)\n",
    "    actual_negative = (y_val == 0)\n",
    "    \n",
    "    predict_positive = (y_pred >= t)\n",
    "    predict_negative = (y_pred < t)\n",
    "\n",
    "    tp = (predict_positive & actual_positive).sum()\n",
    "    tn = (predict_negative & actual_negative).sum()\n",
    "\n",
    "    fp = (predict_positive & actual_negative).sum()\n",
    "    fn = (predict_negative & actual_positive).sum()\n",
    "    \n",
    "    scores.append((t, tp, fp, fn, tn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76beba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['threshold', 'tp', 'fp', 'fn', 'tn']\n",
    "df_scores = pd.DataFrame(scores, columns=columns)\n",
    "\n",
    "df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)\n",
    "df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_scores.threshold, df_scores['tpr'], label='TPR')\n",
    "plt.plot(df_scores.threshold, df_scores['fpr'], label='FPR')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09dc644",
   "metadata": {},
   "source": [
    "### Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "y_rand = np.random.uniform(0, 1, size=len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c24296",
   "metadata": {},
   "outputs": [],
   "source": [
    "((y_rand >= 0.5) == y_val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_fpr_dataframe(y_val, y_pred):\n",
    "    scores = []\n",
    "\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "\n",
    "    for t in thresholds:\n",
    "        actual_positive = (y_val == 1)\n",
    "        actual_negative = (y_val == 0)\n",
    "\n",
    "        predict_positive = (y_pred >= t)\n",
    "        predict_negative = (y_pred < t)\n",
    "\n",
    "        tp = (predict_positive & actual_positive).sum()\n",
    "        tn = (predict_negative & actual_negative).sum()\n",
    "\n",
    "        fp = (predict_positive & actual_negative).sum()\n",
    "        fn = (predict_negative & actual_positive).sum()\n",
    "\n",
    "        scores.append((t, tp, fp, fn, tn))\n",
    "\n",
    "    columns = ['threshold', 'tp', 'fp', 'fn', 'tn']\n",
    "    df_scores = pd.DataFrame(scores, columns=columns)\n",
    "\n",
    "    df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)\n",
    "    df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)\n",
    "    \n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rand = tpr_fpr_dataframe(y_val, y_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea930c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_rand.threshold, df_rand['tpr'], label='TPR')\n",
    "plt.plot(df_rand.threshold, df_rand['fpr'], label='FPR')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958bf4d",
   "metadata": {},
   "source": [
    "### Ideal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neg = (y_val == 0).sum()\n",
    "num_pos = (y_val == 1).sum()\n",
    "num_neg, num_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f051585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_ideal = np.repeat([0, 1], [num_neg, num_pos])\n",
    "y_ideal\n",
    "\n",
    "y_ideal_pred = np.linspace(0, 1, len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - y_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85adf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_ideal, y_ideal_pred >= 0.726)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30738fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ideal = tpr_fpr_dataframe(y_ideal, y_ideal_pred)\n",
    "df_ideal[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_ideal.threshold, df_ideal['tpr'], label='TPR')\n",
    "plt.plot(df_ideal.threshold, df_ideal['fpr'], label='FPR')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d98c9",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a28486",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_scores.threshold, df_scores['tpr'], label='TPR', color='black')\n",
    "plt.plot(df_scores.threshold, df_scores['fpr'], label='FPR', color='blue')\n",
    "\n",
    "plt.plot(df_ideal.threshold, df_ideal['tpr'], label='TPR ideal')\n",
    "plt.plot(df_ideal.threshold, df_ideal['fpr'], label='FPR ideal')\n",
    "\n",
    "# plt.plot(df_rand.threshold, df_rand['tpr'], label='TPR random', color='grey')\n",
    "# plt.plot(df_rand.threshold, df_rand['fpr'], label='FPR random', color='grey')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc605391",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.plot(df_scores.fpr, df_scores.tpr, label='Model')\n",
    "plt.plot([0, 1], [0, 1], label='Random', linestyle='--')\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a689523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c94946",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.plot(fpr, tpr, label='Model')\n",
    "plt.plot([0, 1], [0, 1], label='Random', linestyle='--')\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f8028",
   "metadata": {},
   "source": [
    "## 4.6 ROC AUC\n",
    "\n",
    "* Area under the ROC curve - useful metric\n",
    "* Interpretation of AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c0c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(df_scores.fpr, df_scores.tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(df_ideal.fpr, df_ideal.tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8658f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred)\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cbdd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd31898",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb73e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = y_pred[y_val == 0]\n",
    "pos = y_pred[y_val == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7da8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000\n",
    "success = 0 \n",
    "\n",
    "for i in range(n):\n",
    "    pos_ind = random.randint(0, len(pos) - 1)\n",
    "    neg_ind = random.randint(0, len(neg) - 1)\n",
    "\n",
    "    if pos[pos_ind] > neg[neg_ind]:\n",
    "        success = success + 1\n",
    "\n",
    "success / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50000\n",
    "\n",
    "np.random.seed(1)\n",
    "pos_ind = np.random.randint(0, len(pos), size=n)\n",
    "neg_ind = np.random.randint(0, len(neg), size=n)\n",
    "\n",
    "(pos[pos_ind] > neg[neg_ind]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6336a636",
   "metadata": {},
   "source": [
    "## 4.7 Cross-Validation\n",
    "\n",
    "* Evaluating the same model on different subsets of data\n",
    "* Getting the average prediction and the spread within predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92708443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "\n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4689044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv, model = train(df_train, y_train, C=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, dv, model):\n",
    "    dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    X = dv.transform(dicts)\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d88ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(df_val, dv, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d8a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c8e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce936aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "\n",
    "for C in tqdm([0.001, 0.01, 0.1, 0.5, 1, 5, 10]):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in kfold.split(df_full_train):\n",
    "        df_train = df_full_train.iloc[train_idx]\n",
    "        df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "        y_train = df_train.churn.values\n",
    "        y_val = df_val.churn.values\n",
    "\n",
    "        dv, model = train(df_train, y_train, C=C)\n",
    "        y_pred = predict(df_val, dv, model)\n",
    "\n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(auc)\n",
    "\n",
    "    print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e81326",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db242dff",
   "metadata": {},
   "source": [
    "## 4.8 Summary\n",
    "\n",
    "* Metric - a single number that describes the performance of a model\n",
    "* Accuracy - fraction of correct answers; sometimes misleading \n",
    "* Precision and recall are less misleading when we have class inbalance\n",
    "* ROC Curve - a way to evaluate the performance at all thresholds; okay to use with imbalance\n",
    "* K-Fold CV - more reliable estimate for performance (mean + std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941b0ca",
   "metadata": {},
   "source": [
    "## 4.9 Explore more\n",
    "\n",
    "* Check the precision and recall of the dummy classifier that always predict \"FALSE\"\n",
    "* F1 score = 2 * P * R / (P + R)\n",
    "* Evaluate precision and recall at different thresholds, plot P vs R - this way you'll get the precision/recall curve (similar to ROC curve)\n",
    "* Area under the PR curve is also a useful metric\n",
    "\n",
    "Other projects:\n",
    "\n",
    "* Calculate the metrics for datasets from the previous week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade60b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
